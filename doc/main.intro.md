# Design overview

The core of kermit is built around the representation of a request to a given URL and 
the implementation of a series of well-defined "state" transitions applied to each of those requests.
A request to a URL is represented by the [CrawlRequest](../../class/CrawlRequest.html).

The "states" are referred to as **processing phase** and each transition moves the request 
further down from phase INITIAL to phase COMPLETE. 

## Processing Phases

All defined processing phases as well as their allowed transitions are illustrated in the following diagram.

```txt
 .-------------.
 |   INITIAL   |
 |-------------|
 | Unprocessed |
 |             |
 '-------------'   \
        |           \
        |            \
        |             v
        v             .--------------------.
 .-------------.      |  ERROR | CANCELED  |      .-----------.
 |  SPOOLED    |      |--------------------|      | COMPLETED |
 |-------------|  --->| - Error            |      |-----------|
 | Waiting for |      | - Duplicate        |      | Done!     |
 | free slot   |      | - Blacklisted etc. |      |           |
 '-------------'      '--------------------'      '-----------'
        |             ^         ^          ^            ^
        |            /          |           \           |
        |           /           |            \          |
        v          /                          \         |
 .-------------.         .-------------.          .-----------.
 |    READY    |         |  FETCHING   |          |  FETCHED  |
 |-------------|         |-------------|          |-----------|
 | Ready for   |-------->| Response    |--------->| Content   |
 | fetching    |         | streaming   |          | received  |
 '-------------'         '-------------'          '-----------'


```

For each processing phase all matching handlers are called to do their processing logic with the request.

All requests are held in a queue and the node.js event loop is used to to schedule processing callbacks for 
the requests that are at the head of the queue.

## Processing Extensions
[Extension](../../class/Extension.html)s are reusable components designed to accomplish specific tasks like storing content 
on local file system or scanning content for certain words. Extensions can attach handlers to 
any of the processing phases. Most of Kermit's core functionality is implemented based on this extension
mechanism. Core extensions provide functionality for request filtering, rate limiting and throttling

# Tutorial

## Instantiation

Instantiation of a Crawler is very simple as Kermit comes with a lot of default options.
An absolute minimal example looks like this:

```cs
# Require the main class from the modules package
{Crawler} = require '../kermit/kermit.modules.coffee'

# This will initialize a crawler with default options and no particularly interesting functionality
Kermit = new Crawler
  name: "downloader" 
# This will issue a request and then detect that there is not much to do with the result
# so the program finishes
Kermit.schedule("http://www.yourtargeturl.info")
    
```

A more elaborate and useful example could look like this:

```cs

# Require the main class and extensions
{Crawler, ext} = require '../kermit/kermit.modules.coffee'
{ResourceDiscovery, Monitoring, OfflineStorage} = ext

# Configure a crawler with some useful extensions
Kermit = new Crawler
  name: "downloader"
  extensions : [
    new OfflineStorage # Add storage to local file system
    new Monitoring # Add regular computation of runtime statistics to log level INFO
    new ResourceDiscovery # Add discovery of href and other resources
  ]
# This will start the crawling process
# The resource discovery extension will scan all html files for links and schedule new requests for each
# NOTE: This program might never stop crawling so maybe you want to add some boundaries
Kermit.schedule("http://www.yourtargeturl.info")
    
```

An example that is likely to finish crawling after all allowed URLs have been visited looks like this:

```cs

# Require the main class and extensions
{Crawler, ext} = require '../kermit/kermit.modules.coffee'
{ResourceDiscovery, Monitoring, OfflineStorage} = ext

# Configure a crawler with some useful extensions
Kermit = new Crawler
  name: "download-apidocs"
  extensions : [
    new OfflineStorage # Add storage to local file system
    new Monitoring # Add regular computation of runtime statistics to log level INFO
    new ResourceDiscovery # Add discovery of href and other resources
  ]
  Options:
    Filtering:
        allow : [
          /.*apidocs\.info.*/
        ]
        
# This will initiate the crawling process
# All discovered URLs outside of the domain 'apidocs.info' will not be executed
# As long as URLs under apidocs.info do not contain autogenerated ids, crawling will
# eventually finish 
Kermit.schedule("http://www.apidocs.info")
    
```

### Options
Most of the features provided by Kermit are implemented as extensions. Each of those extensions
offers some options to configure its behaviour. To pass options down to the core extensions use the
'Options' property of the configuration object.


```cs

Kermit = new Crawler
  name: "download-apidocs"
  Options : 
    Filtering: ... # Options for request filtering go here
    Logging: ... # You can pass a log configuration of your choice
    Queuing: ... # Add rate limits and other features as provided by the queuing system
  
```

### Log configuration
The [LogHub](../../class/LogHub.html) is initialized with a log configuration that tells it what appenders
and log formats to use.

```cs

Kermit = new Crawler
  name: "download-apidocs"
  Options : 
    Logging:
      levels : ['trace', 'info', 'error', 'warn']
      destinations: [
        {
          appender:
            type : 'console'
          levels : ['trace', 'error', 'info', 'warn']
        },
        {
        appender :
              type : 'file'
              filename : "/tmp/crawler/logs/full.log"
            levels: ['trace', 'error', 'info', 'debug', 'warn']
        }    
      ]
  
```

There are a number of [predefined log configurations](../../file/src/kermit/Logging.conf.coffee.html) that can be used.
It is also easily possible to roll up a custom configuration following the code examples from that file.

## Scheduling of URLs
[Coming soon]


## Core extensions

* [QueueWorker](../../class/QueueWorker.html)
* [RequestFilter](../../class/RequestFilter.html)
* [RequestStreamer](../../class/RequestStreamer.html)

## Feature extensions

* [ResourceDiscovery](../../class/RequestStreamer.html)
* [Monitoring](../../class/RequestStreamer.html)
* [OfflineStorage](../../class/RequestStreamer.html)
* [OfflineServer](../../class/RequestStreamer.html)


## Custom Extensions
[Coming soon]